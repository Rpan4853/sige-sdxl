{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bf73e7-51c0-46d0-930e-33ad43e806b0",
   "metadata": {},
   "source": [
    "# SIGE Tiling-based Sparse Convolution Usage Example\n",
    "In this notebook, we will show how to implement a minimal tiling-based sparse convolution with SIGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75daeef6-478b-4de2-9798-30568d64a80d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "1. Install [PyTorch](https://pytorch.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81540656-2076-4eb5-9c6f-27907ded9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77913915-3bca-4241-aa85-a029515a3332",
   "metadata": {},
   "source": [
    "2. Install [SIGE](https://github.com/lmxyy/sige-dev/) and other dependencies. **(This may take several minute.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d28f15-c391-49d5-9b75-b90561f2ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sige \n",
    "!pip install torchprofile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f796ef7-47d6-4de5-93d1-10239b30e32b",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3178f1-d543-40e6-bafa-140c22b3f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchprofile import profile_macs\n",
    "\n",
    "from sige.nn import Gather, Scatter, SIGEConv2d, SIGEModel, SIGEModule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0d911-1179-4cc4-bc18-4771236b75ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Inputs\n",
    "#### Set the test device and generate the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d455743-3b2e-4c78-9807-93aba04106f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "original_input = torch.randn((1, 16, 256, 256), device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60738dbd-2d04-4b8e-acf4-5e955cff7df1",
   "metadata": {},
   "source": [
    "#### Get the difference mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaae464-2d91-4c09-a828-38d3ad703517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import download_url_to_file\n",
    "\n",
    "if not os.path.exists(\"assets/mask.npy\"):\n",
    "    os.makedirs(\"assets\", exist_ok=True)\n",
    "    download_url_to_file(\"https://github.com/lmxyy/sige/blob/main/assets/mask.npy?raw=true\", \"assets/mask.npy\")\n",
    "mask = np.load(\"assets/mask.npy\")\n",
    "\n",
    "mask_image = Image.fromarray(~mask)\n",
    "mask = torch.from_numpy(mask).to(device)\n",
    "display(mask_image)\n",
    "print(\"Difference Mask Sparsity: %.2f%%\" % (mask.sum() / mask.numel() * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a5ff5-8600-4231-b2a1-de4cc333f67e",
   "metadata": {},
   "source": [
    "#### Generate the edited input according to the difference mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac302d-bddf-4f1c-8642-39724fbe21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_input = original_input + torch.randn((1, 16, 256, 256), device=device) * mask[None, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b915d3-8ad0-4bb7-bc58-2cf0e4753f83",
   "metadata": {},
   "source": [
    "### Get the Model\n",
    "We first define a module consisting of a single `Gather`, 3x3 conv and `Scatter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1e2c0-bdab-4d81-85e5-a0a4937887b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModule(SIGEModule):\n",
    "    def __init__(self):\n",
    "        super(ExampleModule, self).__init__()\n",
    "        self.conv = SIGEConv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.gather = Gather(self.conv, block_size=6)\n",
    "        self.scatter = Scatter(self.gather)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gather(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.scatter(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705663a1-bd73-4f30-bee2-75fb9b9a6f71",
   "metadata": {},
   "source": [
    "`SIGEModule` is a `nn.Module` wrapper that supports inference with three different modes:\n",
    "* `full`: The original inference. For the example above, the full mode will just perform the stardard $3 \\times 3$ convolution.\n",
    "* `sparse`: The tiling-based sparse convolution. \n",
    "* `profile`: This mode is only used when profiling the MACs of the tiling-based convolution.\n",
    "It also supports setting the difference mask.\n",
    "\n",
    "`Gather`, `Scatter` and `SIGEConv2d` are also `SIGEModule`. Specifically,\n",
    "* `Gather` initialization requires the paired convolution and the sparse block size. During `full` inference, it will just record the input shape. During `sparse` inference, it will gather the active blocks according to the `active_indices` reduced from the difference mask. During `profile` inference, it will just create a dummy tensor to symbolicly track the computation graph for MACs profiling.\n",
    "* `Scatter` initialization requires the paired `Gather` module. During `full` inference, it will just cache the input tensor. During `sparse` inference, it will scatter the input blocks to the cached tensor according the `active_indices` in the paired `Gather`. During `profile` inference, it will just create a dummy tensor to symbolicly track the computation graph for MACs profiling.\n",
    "* `SIGEConv2d` is just a wrapper of `nn.Conv2d`. During `full` inference, it performs as the standard convolution. During `sparse` or `profile` inference, the `padding` will be 0 as the gathered blocks are already padded.\n",
    "\n",
    "Then we wrap the `ExampleModule` into a `SIGEModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe6769-73b1-4d50-9d81-0a9cffb9e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModel(SIGEModel):\n",
    "    def __init__(self):\n",
    "        super(ExampleModel, self).__init__()\n",
    "        self.example_module = ExampleModule()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.example_module(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193f9b9-8a6f-48a3-b9bf-cdb0ae882d15",
   "metadata": {},
   "source": [
    "`SIGEModel` is a class to wrap the toppest `nn.Module`. It supports setting difference masks and the inference mode to its children `SIGEModule`.\n",
    "\n",
    "Then we can get the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bad44-8644-4f2e-85e8-19cc76de1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExampleModel().to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab6153-c605-4ea8-9950-9e2a1827d987",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "First, let's get the results of the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c80a28-789f-4dee-af46-ce795d80d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.set_mode(\"full\")\n",
    "    std_output = model(edited_input)  # for further comparisons\n",
    "    full_macs = profile_macs(model, (edited_input,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4e228-f843-4781-ad3c-d1ee4e0cc15a",
   "metadata": {},
   "source": [
    "Let's try the sparse  inference with SIGE. We first need to cache the original input results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c84273-c658-4d90-9ecd-b58c4187de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.set_mode(\"full\")\n",
    "    original_output = model(original_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137f35a-921d-41ac-bbcb-5ac7de643a37",
   "metadata": {},
   "source": [
    "Then we could try the sparse inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daba0ad-2695-4949-a8cf-f16c0220ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.set_mode(\"sparse\")\n",
    "    model.set_masks({(256, 256): mask})\n",
    "    sige_output = model(edited_input)\n",
    "    model.set_mode(\"profile\")\n",
    "    sige_macs = profile_macs(model, (edited_input,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebf775-86f9-49d5-a6dd-c1504fa2fe44",
   "metadata": {},
   "source": [
    "`set_masks` takes a `Dict` object as input. The key is the resolution tuple and the value is the 2D mask tensor. Remember that `SIGEModel` will broadcast the masks to all its children `SIGEModule`, including `Gather`. `Gather` will reduce the mask of the corresponding resolution to `active_indices`. \n",
    "\n",
    "Now let's compare the results of the stardard convolution and SIGE sparse convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f58a3d-c907-4a9a-b026-adbf045f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Error: %.6f\" % abs(std_output - sige_output).max().item())\n",
    "print(\"Masked Region: %.2f%%\" % (mask.sum() / mask.numel() * 100).item())\n",
    "print(\"Full MACs: %.2fM\" % (full_macs / 1e6))\n",
    "print(\"SIGE MACs: %.2fM\" % (sige_macs / 1e6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5998558-0db6-4bef-b72e-2ec5896ae74e",
   "metadata": {},
   "source": [
    "SIGE reduces $5.23\\times$ computations in this example. Please refer to our [diffusion model](https://github.com/lmxyy/sige/tree/main/diffusion) and [GauGAN](https://github.com/lmxyy/sige-dev/tree/main/gaugan) benchmark for more usage examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sige')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0cf6e41b72e205a83a845f94a2bdbace9441702cc85c291a6aa6c207869c7c89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
